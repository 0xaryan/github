{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex Networks\n",
    "\n",
    "Student: Hossein Ebrahimpour\n",
    "\n",
    "ID: 98723249\n",
    "\n",
    "Homework 1a - **Question 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from hazm import *\n",
    "import matplotlib as plt\n",
    "import string\n",
    "\n",
    "def replaceLetters(text):\n",
    "    return text.replace(\"آ\", \"ا\").replace(\"...\", \"\").replace(\"…\", \"\")\n",
    "\n",
    "def removeSymbols(items):\n",
    "    symbols = list(\"«».?!؟+-()!*&:–-،,؛\\\"'^%$#@/<>_\")\n",
    "    return list(filter(lambda x: x not in symbols, items))\n",
    "\n",
    "def removeStopwords(items):\n",
    "    stops = stopwords_list() + [\"http\", \"https\", \"twitter\"] + [\"این\", \"ان\"]\n",
    "    return list(filter(lambda x: x not in stops, items))\n",
    "\n",
    "def makeGraph(text, clean = False, drawGraph = False):\n",
    "    text = Normalizer().normalize(text)\n",
    "    text = replaceLetters(text)\n",
    "    tokenizer = WordTokenizer(join_verb_parts=False, replace_links=True)\n",
    "    words = tokenizer.tokenize(text)\n",
    "    if(clean):\n",
    "        # stemmer = Stemmer()\n",
    "        # lemmetizer = Lemmatizer()\n",
    "        #words = list(map(lambda x: stemmer.stem(x), words))\n",
    "        words = removeStopwords(words)\n",
    "    \n",
    "    words = removeSymbols(words)\n",
    "    g = nx.Graph().to_directed()\n",
    "    for i, word in enumerate(words):\n",
    "        if(i + 1 == len(words)):\n",
    "            break;\n",
    "        nextWord = words[i+1]\n",
    "        if((word, nextWord) in g.edges):\n",
    "            edge = g.edges[word, nextWord]\n",
    "            wi = edge['weight']\n",
    "            g.edges[word, nextWord]['weight'] = wi + 1\n",
    "        else:\n",
    "            g.add_weighted_edges_from([(word, words[i+1], 1)])\n",
    "    \n",
    "    if(drawGraph):\n",
    "        nx.draw(g, with_labels=True)\n",
    "        \n",
    "    return (g)\n",
    "\n",
    "def keyPhraseList(wordsGraph):\n",
    "    weights = wordsGraph.edges.data('weight')\n",
    "    return list(sorted(weights, key=lambda x: x[2], reverse=True))\n",
    "\n",
    "def debugKeyPhraseList(wordsGraph):\n",
    "    top10 = list(keyPhraseList(wordsGraph))[0:15]\n",
    "    for (w1, w2, wi) in top10:\n",
    "        print(f\"# Phrase '{w1} {w2}' with weight {wi}\")\n",
    "\n",
    "def addToStartMatches(parts, term, toAdd):\n",
    "    success = False\n",
    "    for words in parts:\n",
    "        if words[0] == term:\n",
    "            words.insert(0, toAdd)\n",
    "            success = True\n",
    "    return success\n",
    "\n",
    "def addToEndMatches(parts, term, toAdd):\n",
    "    success = False\n",
    "    for words in parts:\n",
    "        if words[-1] == term:\n",
    "            words.append(toAdd)\n",
    "            success = True\n",
    "    return success\n",
    "\n",
    "def keyPhrase(wordsGraph, input_threshould=6, output_threshould=3):\n",
    "    keyList = keyPhraseList(wordsGraph)\n",
    "    phraseParts = list() # list of lists\n",
    "    for (w1, w2, wi) in keyList[0:input_threshould]:\n",
    "        success1 = addToEndMatches(phraseParts, w1, w2)\n",
    "        success2 = False\n",
    "        if not success1:\n",
    "            success2 = addToStartMatches(phraseParts, w2, w1)\n",
    "        if not success1 and not success2:\n",
    "            phraseParts.append([w1, w2])\n",
    "    \n",
    "    answerCanditates = list(filter(lambda x: len(x) > output_threshould, phraseParts))\n",
    "    if(len(answerCanditates) == 0):\n",
    "        answerCanditates = phraseParts\n",
    "    return \" \".join(list(map(lambda x: \" \".join(x), answerCanditates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets of data t012010.csv loaded.\n",
      "Tweets of data t022020.csv loaded.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "data1_path = \"t012010.csv\"\n",
    "data2_path = \"t022020.csv\"\n",
    "\n",
    "# load data of first csv\n",
    "tweets1_text = \"\"\n",
    "with open(data1_path, newline='', encoding='utf-8') as csvfile1:\n",
    "    tweet_reader = csv.reader(csvfile1)\n",
    "    for row in tweet_reader:\n",
    "        this_tweet = row[1]\n",
    "        tweets1_text = tweets1_text + this_tweet\n",
    "\n",
    "print(f\"Tweets of data {data1_path} loaded.\")\n",
    "        \n",
    "# load data of seconds csv\n",
    "tweets2_text = \"\"\n",
    "with open(data2_path, newline='', encoding='utf-8') as csvfile2:\n",
    "    tweet_reader = csv.reader(csvfile2)\n",
    "    for row in tweet_reader:\n",
    "        try:\n",
    "            this_tweet = row[1]\n",
    "            tweets2_text = tweets2_text + this_tweet\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "print(f\"Tweets of data {data2_path} loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created graph of tweets of data1 without cleaning stopwords.\n",
      "Created graph of tweets of data2 without cleaning stopwords.\n"
     ]
    }
   ],
   "source": [
    "rawGraph1 = makeGraph(tweets1_text)\n",
    "print(\"Created graph of tweets of data1 without cleaning stopwords.\")\n",
    "rawGraph2 = makeGraph(tweets2_text)\n",
    "print(\"Created graph of tweets of data2 without cleaning stopwords.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created graph of tweets of data1, cleaned stopwords.\n",
      "Created graph of tweets of data2, cleaned stopwords.\n"
     ]
    }
   ],
   "source": [
    "cleansedGraph1 = makeGraph(tweets1_text, clean=True)\n",
    "print(\"Created graph of tweets of data1, cleaned stopwords.\")\n",
    "cleansedGraph2 = makeGraph(tweets2_text, clean=True)\n",
    "print(\"Created graph of tweets of data2, cleaned stopwords.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyphrases without removing stop-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key phrase: محاسباتی و اینده نگری و\n"
     ]
    }
   ],
   "source": [
    "key = keyPhrase(rawGraph1)\n",
    "print(f\"Key phrase: {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "عبارت: اصل عدم قطعیت هایزنبرگ\n"
     ]
    }
   ],
   "source": [
    "key = keyPhrase(rawGraph2)\n",
    "print(f\"عبارت: {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key phrases using stopwords removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "عبارت: اینده نگری پز قدرت محاسباتی محاسباتی اینده‌نگری امریکایی‌ها امریکا ضعف\n"
     ]
    }
   ],
   "source": [
    "key = keyPhrase(cleansedGraph1)\n",
    "print(f\"عبارت: {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "عبارت: اصل قطعیت هایزنبرگ براورد قطعیت استاندارد شرایط قطعیت فیزیک کوانتوم\n"
     ]
    }
   ],
   "source": [
    "key = keyPhrase(cleansedGraph2)\n",
    "print(f\"عبارت: {key}\")\n",
    "\n",
    "# Hazm removes \"عدم\" because it is in its stopwords list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit14a36e4cf5e94306a9bc463cbc3e84bf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
